{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Rank .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1wDoCJ6_t9zR-F3FSm41ilYrOblnf5ioD",
      "authorship_tag": "ABX9TyOO2xe/oy3dQt47khu4BEWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talha1503/Extractive_Text_Summarization/blob/master/Text_Rank_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBKp0xYVwZGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GkeGFkJORpN",
        "colab_type": "code",
        "outputId": "933beec8-8c7e-4a91-e920-0980eb0ccd83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v6JIBzRzwyt",
        "colab_type": "code",
        "outputId": "74f39990-464c-45b0-e730-16e0e99ba2a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_DdDijmOeXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "path = '/content/drive/My Drive/Embeddings/glove.840B.300d.pkl'\n",
        "\n",
        "with open(path,'rb') as f:\n",
        "  embeddings = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yUeROgrOhBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "lem = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG0BdFI2OljD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  sentence = re.sub(r'http\\S+',' ',sentence)\n",
        "  sentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n",
        "  sentence = sentence.split()\n",
        "  sentence = [lem.lemmatize(word) for word in sentence if word not in stopwords.words('english')]\n",
        "  sentence = ' '.join(sentence)\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQNRZw3Ota1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_vector(sentence):\n",
        "  words = sentence.split()\n",
        "  size = len(words)\n",
        "  average_vector = np.zeros((size,300))\n",
        "  unknown_words=[]\n",
        "\n",
        "  for index,word in enumerate(words):\n",
        "    try:  \n",
        "        average_vector[index] = embeddings[word].reshape(1,-1)\n",
        "    except Exception as e:\n",
        "      unknown_words.append(word)\n",
        "      average_vector[index] = 0\n",
        "\n",
        "  if size!=0:\n",
        "    average_vector = sum(average_vector)/size\n",
        "  return average_vector,unknown_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIqJTmiuOwnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vector_1,vector_2):\n",
        "  cosine_similarity = 0\n",
        "  try:\n",
        "    cosine_similarity = (np.dot(vector_1,vector_2)/(np.linalg.norm(vector_1)*np.linalg.norm(vector_2)))\n",
        "  except Exception as e :\n",
        "    # print(\"Exception occured\",str(e))\n",
        "    pass\n",
        "  return cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw4v2G6NOzTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_similarity(string1,string2):\n",
        "  # string1,string2 = clean(string1),clean(string2)\n",
        "  vector1,unknown_words1 = average_vector(string1)\n",
        "  vector2,unknown_words2 = average_vector(string2)\n",
        "  similarity = cosine_similarity(vector1,vector2)\n",
        "  return similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvt1rweuO4H4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5785ea1-6abe-4eb0-d547-c89912693f22"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "subject = input(\"Enter the wikipedia topic to be summarised\")\n",
        "base_url = \"https://en.wikipedia.org/wiki/\"+subject\n",
        "page = requests.get(base_url)\n",
        "\n",
        "soup = BeautifulSoup(page.content,'html.parser')\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "content=\"\"\n",
        "for paragraph in paragraphs:\n",
        "    content+=paragraph.text\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the wikipedia topic to be summariseddeep learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGL38jR_PmBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = sent_tokenize(content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDzNoXa_P6h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_sentences=[]\n",
        "for sentence in sentences:\n",
        "  cleaned_sentences.append(clean(sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkekWLWdQFQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity_matrix = np.zeros((len(cleaned_sentences),len(cleaned_sentences)))\n",
        "\n",
        "for i in range(0,len(cleaned_sentences)):\n",
        "  for j in range(0,len(cleaned_sentences)):\n",
        "    if type(find_similarity(cleaned_sentences[i],cleaned_sentences[j])) == np.float64 :\n",
        "      similarity_matrix[i,j] = find_similarity(cleaned_sentences[i],cleaned_sentences[j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH_H_Q0SUjDc",
        "colab_type": "code",
        "outputId": "942d7b1f-d248-4faf-bcc5-0ff56120c2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "similarity_matrix"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.59638508, 0.92437967, ..., 0.73430471, 0.53050314,\n",
              "        0.        ],\n",
              "       [0.59638508, 1.        , 0.51069377, ..., 0.37824843, 0.48714121,\n",
              "        0.        ],\n",
              "       [0.92437967, 0.51069377, 1.        , ..., 0.81988919, 0.5482137 ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.73430471, 0.37824843, 0.81988919, ..., 1.        , 0.58826502,\n",
              "        0.        ],\n",
              "       [0.53050314, 0.48714121, 0.5482137 , ..., 0.58826502, 1.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP_WPKa2TwB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Graph:\n",
        "  \n",
        "  def __init__(self,graph_dictionary):\n",
        "    if not graph_dictionary:\n",
        "      graph_dictionary={}\n",
        "    self.graph_dictionary = graph_dictionary\n",
        "  \n",
        "  def vertices(self):\n",
        "    return self.graph_dictionary.keys()\n",
        "  \n",
        "  def edges(self):\n",
        "    return self.generate_edges()\n",
        "\n",
        "  def add_vertex(self,vertex):\n",
        "    if vertex not in graph_dictionary.keys():\n",
        "      graph_dictionary[vertex] = []\n",
        "  \n",
        "  def add_edge(self,edge):\n",
        "    vertex1,vertex2 = tuple(set(edge))\n",
        "    if vertex1 in graph_dictionary.keys():\n",
        "      graph_dictionary[vertex1].append(vertex2)\n",
        "    else:\n",
        "      graph_dictionary[vertex1] = [vertex2]\n",
        "\n",
        "  def generate_edges(self):\n",
        "    edges = set()\n",
        "    for vertex in graph_dictionary.keys():\n",
        "      for edges in graph_dictionary[vertex]:\n",
        "        edges.add([vertex,edge])\n",
        "    return list(edges)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmD2SVC4ZbIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity_threshold = 0.70\n",
        "network_dictionary = {}\n",
        "\n",
        "for i in range(len(cleaned_sentences)):\n",
        "    network_dictionary[i] = []  \n",
        "\n",
        "for i in range(len(cleaned_sentences)):\n",
        "  for j in range(len(cleaned_sentences)):\n",
        "    if similarity_matrix[i][j] > similarity_threshold:\n",
        "      if j not in network_dictionary[i]:\n",
        "        network_dictionary[i].append(j)\n",
        "      if i not in network_dictionary[j]:\n",
        "        network_dictionary[j].append(i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B-TQfbya-pN",
        "colab_type": "code",
        "outputId": "c70eb0bd-c912-4bc8-ce90-ac925d726af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "similarity_matrix"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.59638508, 0.92437967, ..., 0.73430471, 0.53050314,\n",
              "        0.        ],\n",
              "       [0.59638508, 1.        , 0.51069377, ..., 0.37824843, 0.48714121,\n",
              "        0.        ],\n",
              "       [0.92437967, 0.51069377, 1.        , ..., 0.81988919, 0.5482137 ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.73430471, 0.37824843, 0.81988919, ..., 1.        , 0.58826502,\n",
              "        0.        ],\n",
              "       [0.53050314, 0.48714121, 0.5482137 , ..., 0.58826502, 1.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_bswL7laqpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph = Graph(network_dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v92awKu6b-0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def page_rank(graph,iterations = 50,sentences=20):\n",
        "  ranks = []\n",
        "  # ranks = {}\n",
        "  network = graph.graph_dictionary\n",
        "  current_ranks = np.squeeze(np.zeros((1,len(cleaned_sentences))))\n",
        "  prev_ranks = np.array([1/len(cleaned_sentences)]*len(cleaned_sentences))\n",
        "  for iteration in range(0,iterations):\n",
        "    for i in range(0,len(list(network.keys()))):\n",
        "      current_score = 0\n",
        "      adjacent_vertices = network[list(network.keys())[i]]\n",
        "      for vertex in adjacent_vertices:\n",
        "          current_score += prev_ranks[vertex]/len(network[vertex])\n",
        "      current_ranks[i] = current_score\n",
        "    prev_ranks = current_ranks\n",
        "  \n",
        "  for index in range(len(cleaned_sentences)):\n",
        "      # ranks[index] = prev_ranks[index]\n",
        "      if prev_ranks[index]: \n",
        "        ranks.append((index,prev_ranks[index]))\n",
        "  # ranks = {index:rank for index,rank in sorted(ranks.items(),key=ranks.get,reverse=True)}[:sentences]\n",
        "  ranks = sorted(ranks,key = lambda x:x[1],reverse=True)[:sentences]\n",
        "\n",
        "  return ranks\n",
        "\n",
        "ranks = page_rank(graph,iterations=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSX5ULHlvNkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary = \"\"\n",
        "for index,rank in ranks:\n",
        "  summary+=sentences[index]+\" \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpNeEqVfvmbX",
        "colab_type": "code",
        "outputId": "902a53af-0135-4210-acb7-fdff6f8a9b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "summary"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[206]\\nIn further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[207] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian\\'s[208] website. [75] However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. [170]\\nDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement[171][172]\\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. As Mühlhoff argues, involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as \"human-aided artificial intelligence\". [1][2][3]\\nDeep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. [65][77][75][80]\\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. [65][76] The nature of the recognition errors produced by the two types of systems was characteristically different,[77][74] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. The 2009 NIPS Workshop on Deep Learning for Speech Recognition[74] was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. [1]\\nFor supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. [45][46]\\nSimpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network\\'s (ANN) computational cost and a lack of understanding of how the brain wires its biological networks. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. [209] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[212] decompositions of observed entities and events. Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. [99] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. For example, the computations performed by deep learning units could be similar to those of actual neurons[188][189] and neural populations. [55]\\nMany aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997. [108] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGcXlNEw1UWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}